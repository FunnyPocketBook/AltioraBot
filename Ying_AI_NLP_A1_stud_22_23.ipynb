{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FunnyPocketBook/AltioraBot/blob/master/Ying_AI_NLP_A1_stud_22_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1I7_qy2rYZl"
      },
      "source": [
        "# Lab 1: Text Corpora and Language Modelling\n",
        "\n",
        "Welcome to the first lab assignment of INFOMNLP 2023. \n",
        "\n",
        "This lab is meant to help you get familiar with some language data, and use this data to estimate simple models called N-gram language models.\n",
        "\n",
        "First, you will use the **Penn Treebank**, which is a collection of newspaper articles from the newspaper \n",
        "The Wall Street Journal. The idea is to examine the data and notice interesting properties. This will not take more than a few lines of code. The Penn Treebank may be the most widely used corpus in NLP.\n",
        "\n",
        "Then you will use a corpus consisting of **TedX** talks. This you will use to estimate an **N-gram language model** for different orders of N, and use this for some tasks.\n",
        "\n",
        "The dataset URLs are in the notebook and certain cells are responsible to download them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rules\n",
        "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**.\n",
        "\n",
        "* The **filename** should be the group number, e.g., `01.ipynb` or `31.ipynb`.\n",
        " \n",
        "* The questions marked **Extra** or **Optional** are an additional challenge for those interested in going the extra mile. There are no points for them.\n",
        "\n",
        "**Rules for implementation**\n",
        "\n",
        "* You should **write your code and answers in this iPython Notebook**. (See http://ipython.org/notebook.html for reference material.) If you have problems, please contact your teaching assistant.\n",
        "\n",
        "* Use only **one cell for code** and **one cell for markdown** answers!    \n",
        "\n",
        "    * Put all code in the cell with the `## YOUR CODE HERE ##` comment.\n",
        "    * Provide brief comments on what the code does at crucial points.\n",
        "    * For theoretical questions, put your solution in the `█████ YOUR ANSWER HERE █████` cell.\n",
        "\n",
        "* Don't change or delete any initially provided cells, either text or code, unless explicitly instructed to do so.\n",
        "* Don't delete the comment lines `#TEST...` or edit their code cells. \n",
        "* Don't change the names of provided functions and variables or arguments of the functions. \n",
        "* Leave the output of your code in the output cells.\n",
        "* Don't output unnecessary info (e.g., printing variables for debugging purposes). This clutters the notebook and slows down the grading.\n",
        "* Test your code and **make sure we can run your notebook** in the colab environment.\n",
        "\n",
        "<font color=\"red\">You following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>  "
      ],
      "metadata": {
        "id": "-T11u_DzP4K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "~~Delete this text and write instead of it your:~~\n",
        "* ~~a list of group members names (NOT student IDs)~~\n",
        "* ~~who contributed to which exercises (you don't need to be very detailed)~~ \n"
      ],
      "metadata": {
        "id": "nNT1WNEnlkBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "nQzq7NHLhHAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "L7pnku7cvBpp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns  \n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf data # remove if it was downloaded\n",
        "! wget -nv -P data https://naturallogic.pro/_files_/download/mNLP/PTB/sec02-21.gold.tagged # download the file\n",
        "! wget -nv -P data https://naturallogic.pro/_files_/download/mNLP/PTB/sec00.gold.tagged\n",
        "! wget -nv -P data https://naturallogic.pro/_files_/download/mNLP/TED/ted-train.txt\n",
        "! wget -nv -P data https://naturallogic.pro/_files_/download/mNLP/TED/ted-test.txt"
      ],
      "metadata": {
        "id": "BlFpCuMjgtPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96450f9e-aea8-422a-9142-d7da7973e3a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-12 19:45:11 URL:https://naturallogic.pro/_files_/download/mNLP/PTB/sec02-21.gold.tagged [8158400/8158400] -> \"data/sec02-21.gold.tagged\" [1]\n",
            "2023-05-12 19:45:11 URL:https://naturallogic.pro/_files_/download/mNLP/PTB/sec00.gold.tagged [399917/399917] -> \"data/sec00.gold.tagged\" [1]\n",
            "2023-05-12 19:45:21 URL:https://naturallogic.pro/_files_/download/mNLP/TED/ted-train.txt [24743290/24743290] -> \"data/ted-train.txt\" [1]\n",
            "2023-05-12 19:45:22 URL:https://naturallogic.pro/_files_/download/mNLP/TED/ted-test.txt [25383/25383] -> \"data/ted-test.txt\" [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Golab variables"
      ],
      "metadata": {
        "id": "2xIJozz2Z7VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PTB_FILES = {'train': 'data/sec02-21.gold.tagged', 'dev': 'data/sec00.gold.tagged'}\n",
        "TED_FILES = {'train': 'data/ted-train.txt', 'test': 'data/ted-test.txt'}"
      ],
      "metadata": {
        "id": "8SW7DNhHZ_ay"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYUkG7TKsAAi"
      },
      "source": [
        "# 1. Penn treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flkRSFQtrYZo"
      },
      "source": [
        "## Ex 1.1 [30pt]\n",
        "\n",
        "You are provided with a corpus containing tokens with their Part-of-Speech tags (POS-tags for short). The format is\n",
        "**token|POS** (one sentence per line) and the file name is **sec02-21.gold.tagged**. This data is extracted from Sections 02-21 from the Penn Treebank: these sections are most commonly used for training statistical models like POS taggers and parsers.\n",
        "\n",
        "Note the **distinction between token & word**. \"Token\" is a technical term and represents an occurrence of a word. Here, \"word\" covers both lexical words and other symbols (e.g., punctuations or numbers). Often \"word\" is called \"token type\". So, one can say that a token type can have different occurrences and each occurrence is a token.\n",
        "\n",
        "**[Hint]** **Figure 8.2** in chapter 8 of Jurafsky and Martin (see [here](https://web.stanford.edu/~jurafsky/slp3/8.pdf#page=4)) holds a summary of POS-tags used in the Penn Treebank tagset together with their meaning and some examples.\n",
        "\n",
        "**[Hint]** The Python library [collections](https://docs.python.org/3.7/library/collections.html) has an object called `Counter` which will come in handy for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_md_iporYZs"
      },
      "source": [
        "### (a) Corpus & vocab [5pt]\n",
        "\n",
        "**Print** the corpus size (i.e. the number of **tokens**).  \n",
        "**Print** the size of the **vocabulary** of the corpus. Estimate the vocabulary size both by **lowercasing** all the tokens as well as by leaving the tokens in their **original orthography**.  \n",
        "\n",
        "What is the **advantage** of lowercasing all the tokens in your corpus?  \n",
        "What is a notable **downside**?  \n",
        "**Give examples** of the advantage and the downside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jxrgbpkdvBpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3642c65-5afc-455b-cc85-fa83fff76ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus size: 929552\n",
            "Vocabulary size (original orthography): 44210\n",
            "Vocabulary size (lowercase): 39384\n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "# Use PTB_FILES golab var\n",
        "# TEST: original orthography vocab size = ...10\n",
        "with open(PTB_FILES[\"train\"], \"r\") as f:\n",
        "    corpus = f.read().strip().split()\n",
        "  \n",
        "# Splits the token from POS\n",
        "tokens = [w.split(\"|\")[0] for w in corpus]\n",
        "pos = [w.split(\"|\")[1] for w in corpus]\n",
        "\n",
        "df = pd.DataFrame(list(zip(tokens, pos)), columns=[\"token\", \"pos\"])\n",
        "\n",
        "num_tokens = len(tokens)\n",
        "print(f\"Corpus size: {num_tokens}\")\n",
        "\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "vocab_orig_ortho = set(token_counts.keys())\n",
        "print(f\"Vocabulary size (original orthography): {len(vocab_orig_ortho)}\")\n",
        "\n",
        "vocab_lowercase = set(token.lower() for token in tokens)\n",
        "print(f\"Vocabulary size (lowercase): {len(vocab_lowercase)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4F1hlWNvBpz"
      },
      "source": [
        "█████ YOUR ANSWER HERE █████\n",
        "\n",
        "1. Lowercasing all the tokens in the corpus helps reduce the size of the vocabulary and is one way of mitigating the sparsity problem in NLP tasks.\n",
        "\n",
        "2. A notable downside is that is can lead to the loss of important information that is conveyed by the capitalization of the words.\n",
        "\n",
        "3. - Adavantage: \"the\" and \"The\" would be considered as the same word with lowercasing.\n",
        "  - Disadvantage: \"apple\" (the fruit) and \"Apple\" (the company) have different meanings, but with lowercasing this distinction is gone. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn1cBwu4vBp0"
      },
      "source": [
        "------\n",
        "**For the rest of this exercise <font color=\"red\">you should use the original orthography</font> of the data when answering the questions.**\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D1kQ1N_vBp1"
      },
      "source": [
        "### [Extra] \n",
        "\n",
        "Plot a graph of word frequency versus rank of a word, in this corpus. Does this corpus obey **Zipf’s law**? For a better picture you might need to make the plot horizontally wide or consider an initial cutoff of ranked words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2A_By8MRvBp2",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "3cc71b0a-13d6-418b-81a7-8fe9954edef2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "ordered_tokens = token_counts.most_common()\n",
        "freq = [count for token, count in ordered_tokens]\n",
        "rank = list(range(1, len(ordered_tokens)+1))\n",
        "plt.plot(rank, freq)\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlim(1,400)\n",
        "plt.title(\"Rank-frequency plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "█████ YOUR ANSWER HERE █████\n",
        "\n",
        "The corpus obeys Zipf's law. The highest ~50 ranked words make up the vast majority of the corpus."
      ],
      "metadata": {
        "id": "4IKxQutFd-VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Tags & tokens [5pt]\n",
        "\n",
        "**Print** a list of the **10 most commonly occurring POS tags** in the data. For each of these POS tags, additionally **print** the **3 most common token types** (i.e., words) that belong to that tag. Print raw counts for each printed tag and token type."
      ],
      "metadata": {
        "id": "r1ZeD-QRwVvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LjWaeRNmvBp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a8dfdc-48c9-43d1-f41b-cf89b35ae7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN (132134): %(4866); company(2457); year(2220); \n",
            "IN (99413): of(22778); in(14852); for(7907); \n",
            "NNP (90711): Mr.(4147); U.S.(1577); Corp.(1186); \n",
            "DT (82147): the(40831); a(19151); The(6753); \n",
            "JJ (59643): new(1396); other(1298); last(990); \n",
            "NNS (59332): years(1164); shares(1128); sales(939); \n",
            ", (48314): ,(48310); an(1); 2(1); \n",
            ". (39252): .(38798); ?(392); !(62); \n",
            "CD (36148): million(4355); billion(1780); one(1203); \n",
            "RB (30232): n't(3211); also(1420); not(1287); \n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "# Use the following printing format and test your numbers for the DT pos tag:\n",
        "# DT (82147):\tthe(40831); a(19151); The(6753)\n",
        "\n",
        "most_common_pos = df[\"pos\"].value_counts(ascending=False)[:10]\n",
        "# Iterates through the most common POS\n",
        "for i, pos in enumerate(most_common_pos.index.tolist()):\n",
        "  df_pos = df[df[\"pos\"] == pos] # Get all tokens whose POS tag equals `pos`\n",
        "  token_count = df_pos[\"token\"].value_counts(ascending=False) # Count how many tokens occur for each word with that POS tag\n",
        "  print(f\"{pos} ({most_common_pos[i]}): \" , end=\"\")\n",
        "  for j, token in enumerate(token_count[:3].index.tolist()):\n",
        "    print(f\"{token}({token_count[:3][j]}); \", end=\"\")\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPTQVzwvvBp6"
      },
      "source": [
        "### (c) Ambiguity [10pt]\n",
        "\n",
        "A single token type (i.e. word) may occur with several POS tags. For example, *record* can be both a **noun** *(buy a record)* or a **verb** *(record a lecture)*. This makes POS tags extremely useful for **disambiguation**.\n",
        "\n",
        "**Print** the percentage(!) of the **ambiguous** words in the vocabulary(!) (i.e. token types that have more than one POS tag?).  \n",
        "\n",
        "Ambiguous words do not account for a great percentage of the vocabulary. Yet they are among the most commonly occurring words in the English language.  \n",
        "**Print** the percentage of the dataset that is ambiguous (i.e., counting tokens of ambiguous token types/words wrt data size).  \n",
        "(Don't round percentages when printing)\n",
        "\n",
        "**Print** the **10 most frequently shared POS tag pairs** (i.e. POS tag pairs that share most words).  \n",
        "**Print** the **4 most ambiguous** words (i.e. based on the number of POS tags it can get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8qNODIAcvBp6",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015ac1b8-9c96-4f33-fccc-ded1a665deb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of ambiguous words in the vocabulary: 13.469803211942999%\n",
            "Percentage of dataset that is ambiguous: 55.079005800643756%\n",
            "Top 10 confused tag pairs\n",
            "(VBD, VBN) share 1060 tokens\n",
            "(JJ, NN) share 962 tokens\n",
            "(VB, NN) share 775 tokens\n",
            "(VB, VBP) share 752 tokens\n",
            "(NN, NNP) share 627 tokens\n",
            "(JJ, VBN) share 456 tokens\n",
            "(VBP, NN) share 403 tokens\n",
            "(VBG, NN) share 378 tokens\n",
            "(NNS, VBZ) share 364 tokens\n",
            "(JJ, NNP) share 297 tokens\n",
            "Top 4 ambiguous tokens:\n",
            "many: [JJ, NN, RB, DT, PDT, VB, NP]\n",
            "down: [RB, RP, IN, JJ, RBR, VBP, NN]\n",
            "set: [VBN, NN, VBD, VB, VBP, JJ, VBZ]\n",
            "open: [VB, JJ, RP, RB, VBP, NN, VBZ]\n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "# TEST: There are 1060 tokens whose tokens get both ('VBD', 'VBN') tags\n",
        "# TEST: in other tokens, a set of tokens with VBD and a set of tokens with VBN share 1060 elements \n",
        "\n",
        "# print in the following way:\n",
        "# Print ambiguous tokens and tag pairs as, where tags are ordered alphabetically:\n",
        "# Top 10 confused tag pairs:\n",
        "#\t('VBD', 'VBN') share 1060 tokens \n",
        "#   ...\n",
        "# Top 4 ambiguous tokens:\n",
        "# \ttoken: [POS_1, ..., POS_n]\n",
        "#   ...\n",
        "\n",
        "import itertools\n",
        "\n",
        "df_ambiguous = df.groupby(\"token\")[\"pos\"].unique().reset_index() # Combine the POS tags for tokens of the same word into a list\n",
        "df_ambiguous = df_ambiguous[df_ambiguous[\"pos\"].str.len() > 1] # Keep only the rows where there is more than one POS tag for the word\n",
        "df_ambiguous_tokens = df[df[\"token\"].isin(df_ambiguous[\"token\"])] # Get all tokens that belong to an ambiguous word\n",
        "print(f\"Percentage of ambiguous words in the vocabulary: {len(df_ambiguous) / len(df['token'].unique()) * 100}%\")\n",
        "print(f\"Percentage of dataset that is ambiguous: {len(df_ambiguous_tokens) / len(df) * 100}%\")\n",
        "\n",
        "pos_pairs = itertools.combinations(df[\"pos\"].unique().tolist(), 2) # Get all POS tag pairs\n",
        "pos_pairs = dict.fromkeys(map(lambda x: frozenset(x), pos_pairs), 0) # Create a dict from the pairs. Turn them into a frozenset to be able to use the pair as key\n",
        "# For each POS pair, check the POS tag list of each word to see if the POS pair is included in the list and count how many there are\n",
        "for pair in pos_pairs:\n",
        "  mask = df_ambiguous['pos'].apply(lambda x: frozenset.intersection(pair, x) == pair)\n",
        "  filtered_df = df_ambiguous[mask]\n",
        "  pos_pairs[pair] = len(filtered_df) \n",
        "\n",
        "print(f\"Top 10 confused tag pairs\")\n",
        "for pair, count in Counter(pos_pairs).most_common(10):\n",
        "  print(f\"({', '.join(list(pair))}) share {count} tokens\")\n",
        "\n",
        "print(\"Top 4 ambiguous tokens:\")\n",
        "for row in df_ambiguous.sort_values(by=\"pos\", key=lambda x: x.str.len(), ascending=False)[:4].itertuples():\n",
        "  print(f\"{row.token}: [{', '.join(row.pos)}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODJl38yvBp7"
      },
      "source": [
        "### (d) Explain [10pt]\n",
        "\n",
        "**Take one** of the 4 most ambiguous tokens and for **each** of its possible POS tags **give** a sentence/phrase that shows the use of the token with its POS tag.  \n",
        "For the other **three** tokens, **discuss one** POS tag that is most unlikely for the corresponding token.      \n",
        "Consult [Figure 8.2](https://web.stanford.edu/~jurafsky/slp3/8.pdf#page=4) for the interpretation of the POS tags. If you would like a more detailed explanation of the POS tags, consult [this technical report](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&context=cis_reports). Of course, you don't need to read it properly, just use it to search for certain POS tags in it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "█████ YOUR ANSWER HERE █████\n",
        "\n",
        "set: \n",
        "  - NN: {} is the empty set.\n",
        "  - VBP: I set the mug on the table.\n",
        "  - VB: I will set the mug on the table.\n",
        "  - JJ: The mug costs a set price.\n",
        "  - VBZ: not possible\n",
        "  - VBD: Yesterday, I set the mug on the table.\n",
        "  - VBN: Yesterday, I have set the mug on the table.\n",
        "\n",
        "many: VB is most unlikely, as many cannot be a verb.\n",
        "down: RBR is most unlikely, as \"down\" itself is not a comparative adverb\n",
        "open: VBZ, as 3rd person singular present is \"opens\""
      ],
      "metadata": {
        "id": "c18r5YJM7xDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9fwBwWBrYZ3"
      },
      "source": [
        "## Ex 1.2 [10pt] \n",
        "\n",
        "You are also provided with another file called **sec00.gold.tagged**. \n",
        "Section 00 of the Penn Treebank is typically used as development data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1GVBPzvvBp9"
      },
      "source": [
        "### (a) Unseen % [5pt]\n",
        "\n",
        "**Print** the percentages of the **unseen development vocabulary** and **unseen development data** (i.e., token types and tokens that occur in the development data but not in the training data). Percentages should be calculated wrt the development vocabulary/data size and they shouldn't be rounded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "o7Fjx4q9vBp9",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a39742-ea46-485c-e608-c8fd3cd96a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unseen development vocabulary percentage: 16.35%\n",
            "Unseen development data percentage: 3.80%\n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "# Use PTB_FILES global var\n",
        "# TEST: Difference between the %age is in interval 11-14%\n",
        "with open(PTB_FILES[\"dev\"], \"r\") as f:\n",
        "    corpus_dev = f.read().strip().split()\n",
        "  \n",
        "# Split the token from POS\n",
        "tokens_dev = [w.split(\"|\")[0] for w in corpus_dev]\n",
        "pos_dev = [w.split(\"|\")[1] for w in corpus_dev]\n",
        "\n",
        "df_dev = pd.DataFrame(list(zip(tokens_dev, pos_dev)), columns=[\"token\", \"pos\"])\n",
        "\n",
        "# create sets of the unique tokens in the training and development sets\n",
        "train_tokens = set(df['token'])\n",
        "dev_tokens = set(df_dev['token'])\n",
        "\n",
        "# calculate the sizes of the training and development sets\n",
        "train_size = len(train_tokens)\n",
        "dev_size = len(dev_tokens)\n",
        "\n",
        "# find the tokens that are unique to the development set\n",
        "dev_unseen_tokens = dev_tokens - train_tokens\n",
        "\n",
        "# calculate the percentages of the unseen development vocabulary and data\n",
        "dev_unseen_vocab_percent = len(dev_unseen_tokens) / dev_size * 100\n",
        "dev_unseen_data_percent = len(df_dev[~df_dev['token'].isin(train_tokens)]) / len(df_dev) * 100\n",
        "\n",
        "# print the results\n",
        "print(f\"Unseen development vocabulary percentage: {dev_unseen_vocab_percent:.2f}%\")\n",
        "print(f\"Unseen development data percentage: {dev_unseen_data_percent:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXDWzuxvBp-"
      },
      "source": [
        "### (b) Unseen tok tag [5pt]\n",
        "\n",
        "**Print** the top **five** POS tags that the most **unseen tokens** belong to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VgbUb8nHvBp-",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e12042-cfb7-475e-8abc-58352d0dc0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "815 NNP\n",
            "258 JJ\n",
            "210 NN\n",
            "146 NNS\n",
            "114 CD\n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "# Print in the following format and test your number for NNS:\n",
        "# 146\tNNS\n",
        "\n",
        "unseen_tokens = df_dev[~df_dev[\"token\"].isin(df[\"token\"].unique())]\n",
        "unseen_tokens_pos = unseen_tokens.groupby(\"pos\").count().reset_index().sort_values(\"token\", ascending=False).head(5)\n",
        "for row in unseen_tokens_pos.itertuples():\n",
        "  print(f\"{row.token} {row.pos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Language Models"
      ],
      "metadata": {
        "id": "6cbdG9BsdRdh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jECmv9_BvBp-"
      },
      "source": [
        "\n",
        "\n",
        "The following questions presuppose J&M's [chapter 3 on n-gram language models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "Models that assign **probabilities** to **sequences of words** are called **language\n",
        "models** or **LMs**. The simplest model that assigns probabilities to sentences and sequences of words is the **N-gram** model.\n",
        "\n",
        "Recall that an *N*-gram language model uses **conditional probabilities** of the form\n",
        "    \n",
        "$$P(w_k \\mid w_{k-N+1} \\dots w_{k-1})$$\n",
        "\n",
        "to **approximate** the full **joint probability**\n",
        "\n",
        "$$P(w_1 \\dots w_n)$$\n",
        "\n",
        "of a sequence of words $w_1 \\dots w_n$.\n",
        "\n",
        "The easiest way of obtaining estimates for the probabilities $P(w_k \\mid w_{k-N+1} \\dots w_{k-1})$ is to use the **maximum likelihood estimate** or **MLE**, a widely used statistical estimation method ([read more](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)). You count and normalize:\n",
        "\n",
        "$$P_{MLE}(w_k \\mid w_{k-N+1} \\dots w_{k-1}) = \\frac{C(w_{k-N+1} \\dots w_{k-1} w_k)}{C(w_{k-N+1} \\dots w_{k-1})}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex 2.1 [25pt] \n",
        "\n",
        "In this exercise you will have to train $N$-gram language models with an optional smoothing."
      ],
      "metadata": {
        "id": "zJJRUQLpQIYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some help\n",
        "\n",
        "The `collections` library has another useful data structure: the `defaultdict`. Some example uses (learn more [here](https://realpython.com/python-defaultdict/) if needed):"
      ],
      "metadata": {
        "id": "Em9s4OTHfYda"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy6bNZpedd4a"
      },
      "source": [
        "**[Datastructure hint]** If you store the smoothed language in a naive manner (that is, to store *all* the numbers separately) your datastructure will get huge! If $V$ is the vocabulary then the smoothed bigram model assigns probabilities to $|V|^2$ entries. If $|V|$ is around 80k, the naive way requires you to store more than 64 billion floats. Yet almost all of these are actually just $P(w_n|w_{n-1}) = \\frac{k}{N + k|V|}$, with $k$ the value with which you smooth and $N=C(w_{n-1})$. Think about how you use this fact to make your model work in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NEzYDa-GvBp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40333e34-7ae9-40d5-d7e9-80e7d82ab037"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "d = defaultdict(float)\n",
        "d[\"new key\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80cGhiN7vBp_"
      },
      "source": [
        "Compare that to an ordinary dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hwPC-F7VvBqA"
      },
      "outputs": [],
      "source": [
        "# d = dict()\n",
        "# d[\"new key\"]\n",
        "# Throws an KeyError: 'new key'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhC9fVSBvBqA"
      },
      "source": [
        "Other datatypes as `default_factory`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ANtXMfCDvBqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0b60d1-4f49-4ee7-a5e1-65ede38bdb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {})\n",
            "0\n",
            "defaultdict(<class 'int'>, {'new key': 0})\n"
          ]
        }
      ],
      "source": [
        "d = defaultdict(int)\n",
        "print(d)\n",
        "print(d[\"new key\"])\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gHIBoHbrvBqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57f4162-b46e-4558-a89f-9f37316b7cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {})\n",
            "[]\n",
            "defaultdict(<class 'list'>, {'new key': []})\n"
          ]
        }
      ],
      "source": [
        "d = defaultdict(list)\n",
        "print(d)\n",
        "print(d[\"new key\"])\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how the defaultdict is modified after getting a value of a non-existing key. If you want to avoid modifying the defaultdict when getting a value for a non-existing key, do the following:"
      ],
      "metadata": {
        "id": "9Obc7SPcRwB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = defaultdict(list)\n",
        "print(d)\n",
        "print(d.get(\"new key\", d.default_factory()))\n",
        "print(d)"
      ],
      "metadata": {
        "id": "Z9YsnMXZRwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6fdbce-98b4-4cde-a2a7-5680d11d3618"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {})\n",
            "[]\n",
            "defaultdict(<class 'list'>, {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReMovWEyvBqC"
      },
      "source": [
        "Converting an already existing `dict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eO-PZAXyvBqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65d3501-9a29-4f53-d6e9-f8465096a38b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "d1 = {k: \"value\" for k in range(1, 11)}\n",
        "d = defaultdict(float, d1) # convert it to a defaultdict\n",
        "print(d[5])\n",
        "print(d[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qup8dlervBqC"
      },
      "source": [
        "This doesn't work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vFdSoDb9vBqD"
      },
      "outputs": [],
      "source": [
        "# d = defaultdict(10)\n",
        "# Throws a TypeError: first argument must be callable or None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4bK1eGvBqD"
      },
      "source": [
        "Use a `lambda` to make the number `10` `callable`\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BS8vozeYvBqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d7698f-35bc-44b9-cd54-761e34691253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function <lambda> at 0x7f38ffba1ee0>, {})\n",
            "10\n",
            "defaultdict(<function <lambda> at 0x7f38ffba1ee0>, {'new key': 10})\n"
          ]
        }
      ],
      "source": [
        "d = defaultdict(lambda: 10)\n",
        "print(d)\n",
        "print(d[\"new key\"])\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vegtS6wJvBqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d44164-59c4-4847-ed0f-1f6041b4ed5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function <lambda> at 0x7f38ffba3160>, {})\n",
            "defaultdict(<class 'float'>, {})\n",
            "defaultdict(<function <lambda> at 0x7f38ffba3160>, {'new key': defaultdict(<class 'float'>, {})})\n"
          ]
        }
      ],
      "source": [
        "d = defaultdict(lambda: defaultdict(float))\n",
        "print(d)\n",
        "print(d[\"new key\"])\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in some cases the behaviour of lambda functions might be counterintuitive. One of such cases is the closure: [link1](https://realpython.com/python-lambda/#closure), [link2](https://stackoverflow.com/questions/2295290/what-do-lambda-function-closures-capture)."
      ],
      "metadata": {
        "id": "ThZCgTQTo4KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a shortcut for the read-only get for defaultdict that supports several keys\n",
        "# i.e. defget(d, [k1, k2]) will return the value of d[k1][k2] without altering d \n",
        "def defget(d, keys):\n",
        "    for k in keys:\n",
        "        d = d.get(k, d.default_factory())\n",
        "    return d\n",
        "\n",
        "# testing if it works as intended\n",
        "d = defaultdict(lambda: defaultdict(int))\n",
        "d[1][1] = 2\n",
        "d[2][1] = 3\n",
        "print(d) \n",
        "print(defget(d, [1, 1]))\n",
        "print(defget(d, [3]))\n",
        "print(defget(d, [3, 1]))\n",
        "print(d)"
      ],
      "metadata": {
        "id": "NKYpXbk2cEw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de04e29-e702-474d-8dc2-e8c69e10bb5d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function <lambda> at 0x7f38ff3f2d30>, {1: defaultdict(<class 'int'>, {1: 2}), 2: defaultdict(<class 'int'>, {1: 3})})\n",
            "2\n",
            "defaultdict(<class 'int'>, {})\n",
            "0\n",
            "defaultdict(<function <lambda> at 0x7f38ff3f2d30>, {1: defaultdict(<class 'int'>, {1: 2}), 2: defaultdict(<class 'int'>, {1: 3})})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WYXdmJXvBqE"
      },
      "source": [
        "Clever use of a `defaultdict` can be the solution to the problem of data-storing in a smoothing $N$-gram pointed out above:\n",
        "    \n",
        "    d = bigram_frequencies[history] = {w1: 120, w2: 340, w3: 7 ...}\n",
        "    N = sum(d.values())\n",
        "    # d_normalized = .... # normalize d with smoothing applied to elements already in the dictionary  \n",
        "    d_smoothed = defaultdict(lambda: k/(N + kV), d_normalized) # add smoothing to 'unseen' words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5svCtCyDvBqE"
      },
      "source": [
        "The following function is given to assist you with reading the data from the file in a convenient format. Understand how the function works and figure out how to use it for training n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1DnP4YcPvBqF"
      },
      "outputs": [],
      "source": [
        "def read_data(fname, h=1, max_lines=np.inf): \n",
        "    \"\"\"\n",
        "    Reads in the data from a file and returns a vocabulary as a set\n",
        "    and the sentence-padded data as a list of list. \n",
        "    \n",
        "    :param fname: path to the file\n",
        "    :param max_lines: the number of top lines to read (can be used for debugging)\n",
        "    :param h: the length of n-gram history\n",
        "    :returns: data as a list of lists and vocabulary as a set    \n",
        "    \"\"\"\n",
        "    data = []\n",
        "    start = h * [\"<s>\"]\n",
        "    end = [\"</s>\"]\n",
        "    vocab = set()\n",
        "    \n",
        "    with open(fname, \"r\") as F:\n",
        "        for k, line in enumerate(F):\n",
        "            # an optional cut-off to read a part of the data\n",
        "            if k > max_lines:\n",
        "                break\n",
        "            words = line.strip().split()\n",
        "            vocab.update(words)\n",
        "            # padding the sentence             \n",
        "            sent = start + words + end\n",
        "            data.append(sent)\n",
        "\n",
        "    return data, vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_ngram\n",
        "\n",
        "[15pt] **Complete** the function `train_ngram` so that you can train a count-based $N$-gram language model on the data found in `data/ted-train.txt` and train this for $N=2,3,4$. \n",
        "\n",
        "[10pt] **Extend** the function `train_ngram` so that it accepts a parameter `k` for optional add-$k$ smoothing (upgrade the same function, don't define new). "
      ],
      "metadata": {
        "id": "3dy2LfV1m3Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "rrddmakLDoEh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UCTtBMxWvBqF"
      },
      "outputs": [],
      "source": [
        "def train_ngram(data, N=2, k=0):\n",
        "    \"\"\"\n",
        "    Trains an n-gram language model with optional add-k smoothing\n",
        "    and additionally returns the unigram model\n",
        "    \n",
        "    :param data: text-data as returned by the pre-defined function read_data\n",
        "    :param N: (N>1) the order of the ngram e.g. N=2 gives a bigram\n",
        "    :param k: optional add-k smoothing\n",
        "    :returns: ngram and unigram\n",
        "    \"\"\"\n",
        "    ngram = defaultdict(Counter) # ngram[history][word] = #(history,word)\n",
        "    # for history of length >1, join tokens with a singel white space (see tests below)\n",
        "\n",
        "    ## YOUR CODE HERE ##\n",
        "    ## you can have auxiliary functions if needed, inside of outside this function's scope\n",
        "    tokens = data.split()\n",
        "\n",
        "    # generate ngram\n",
        "    for i in range(len(tokens) - N + 1):\n",
        "      history, word = tuple(tokens[i:i+N-1]), tokens[i+N-1]\n",
        "      ngram[\" \".join(history)][word] += 1\n",
        "\n",
        "    # the following line might be useful and its use is optional\n",
        "    # unigram = defaultdict(float, Counter(\"SOME VAR HERE\")) # default prob is 0.0        \n",
        "\n",
        "    # generate unigram from ngram\n",
        "    unigram = defaultdict(float, Counter([word for sublist in ngram.values() for word in sublist]))\n",
        "\n",
        "    if k > 0:\n",
        "      vocabulary_size = len(unigram)\n",
        "      for history in ngram:\n",
        "        for word in unigram:\n",
        "          ngram[history][word] += k\n",
        "        \n",
        "        # Normalize probabilities\n",
        "        total = float(sum(ngram[history].values()))\n",
        "        for word in ngram[history]:\n",
        "          ngram[history][word] /= total\n",
        "\n",
        "        # Normalize unigram probabilities\n",
        "        total = float(sum(unigram.values()))\n",
        "        for word in unigram:\n",
        "          unigram[word] = (unigram[word] + k) / (total + k * vocabulary_size)    \n",
        "    return ngram, unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scw9yzDYc91M"
      },
      "outputs": [],
      "source": [
        "# shouldn't take more than 2min\n",
        "Data1, vocab = None, None ## YOUR CODE HERE ##\n",
        "# use the global variable for the file path\n",
        "with open(TED_FILES[\"train\"], \"r\") as f:\n",
        "  Data1 = f.read()\n",
        "\n",
        "vocab = set(Data1.split())\n",
        "\n",
        "# non-smoothed bigram\n",
        "Bigram, Unigram = train_ngram(Data1, N=2, k=0)\n",
        "# smoothed bigram\n",
        "Bigram_sm, Unigram_sm = train_ngram(Data1, N=2, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsS5hpWQvBqH"
      },
      "outputs": [],
      "source": [
        "# might take significant amount of memory\n",
        "# shouldn't take more than 3min\n",
        "Data2, vocab = None, None ## YOUR CODE HERE ##\n",
        "\n",
        "# non-smoothed trigram\n",
        "Trigram, Unigram_ = train_ngram(Data2, N=3, k=0)\n",
        "# smoothed trigram\n",
        "Trigram_sm, Unigram_sm_ = train_ngram(Data2, N=3, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TVjxsuyc91N"
      },
      "outputs": [],
      "source": [
        "#TEST 2.1 \n",
        "#!!! be aware that when getting a value of a non-existing key from defaultdict with d[key] method, \n",
        "# the key gets inserted into the defaultdict. That's why tests uses defget\n",
        "assert defget(Bigram, ['all','people']) > 0\n",
        "assert defget(Bigram, ['all','all']) == 0\n",
        "assert defget(Bigram_sm, ['all','all']) == defget(Bigram_sm, ['all','asdasda'])\n",
        "assert defget(Bigram_sm, ['all','asdasda']) > 0\n",
        "assert defget(Trigram, ['<s> <s>','The']) > 0\n",
        "assert defget(Trigram, ['<s> <s>','sun']) == 0\n",
        "assert np.isclose(sum(Trigram['All the'].values()), 1) #almost 1, but not 1 because of float point rounding\n",
        "assert defget(Trigram_sm, ['<s> <s>','sun']) > 0\n",
        "assert Unigram == Unigram_\n",
        "assert Unigram_sm == Unigram_sm_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHqoEt2zvBqJ"
      },
      "source": [
        "## Ex 2.2 [5pt]\n",
        "\n",
        "You can use an *N*-gram language model to **generate text**. The higher the order *N* the better your model will be able to catch the long-range dependencies that occur in actual sentences and the better your chances are at generating sensible text. But beware: **sparsity** of language data will quickly cause your model to reproduce entire lines from your training data; in such cases, only one $w_k$ was observed for the histories $w_{k-N+1}\\dots w_{k-1}$ in the entire training set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some help\n",
        "\n",
        "**[Hint]** You can use the method of [inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling) to generate a sample from a **categorical distribution**, $p_1\\dots p_k$ such that $p_i \\geq 0$ and $\\sum_{i=1}^k p_i = 1$, as follows:"
      ],
      "metadata": {
        "id": "YbOqa3y5hefa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fman6WFYvBqJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "P = [0.2,0.5,0.2,0.1]\n",
        "\n",
        "def sample(P):\n",
        "    u = random.random() # random number between 0 and 1\n",
        "    p = 0\n",
        "    for i, p_i in enumerate(P):\n",
        "        p += p_i\n",
        "        if p > u: \n",
        "            return i # the first i s.t. p1 + ... + pi > u\n",
        "        \n",
        "print(sample(P))\n",
        "\n",
        "print(Counter([sample(P) for i in range(1000)])) # check to see if the law of large numbers is still true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZBZ2mxTvBqK"
      },
      "source": [
        "Inverse transform sampling in the words of Jurafsky and Martin:\n",
        "\n",
        "> Imagine all the words of the English language covering the probability space\n",
        "between 0 and 1, each word covering an interval proportional to its frequency. We\n",
        "choose a random value between 0 and 1 and print the word whose interval includes\n",
        "this chosen value.\n",
        "\n",
        "(J&M, [section 3.3](https://web.stanford.edu/~jurafsky/slp3/3.pdf#page=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate_sent\n",
        "\n",
        "**Complete** the function `generate_sent`. It takes a language model `lm` and an order `N` and should generate a natural language string(!) (it might not be a grammatical sentence) by **sampling** from the language model."
      ],
      "metadata": {
        "id": "hQiNLmMQhO9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCD2ezj_vBqK"
      },
      "outputs": [],
      "source": [
        "def generate_sent(lm, N):\n",
        "    \"\"\"\n",
        "    Generates sentence from a language model based on N-grams\n",
        "    :param lm: language model based on N-grams\n",
        "    :param N: denoting N-gram parameter\n",
        "    :returns: a string of natural language tokens where tokens are separated with a space\n",
        "    \"\"\"\n",
        "    \n",
        "    ## YOUR CODE HERE ##\n",
        "    ## you can have auxiliary functions if needed, inside of outside this function's scope\n",
        "\n",
        "\n",
        "    \n",
        "    raise NotImplementedError(\"Function not implemented yet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0yKCR2xvBqL"
      },
      "outputs": [],
      "source": [
        "#TEST Ex2.2\n",
        "random.seed(42) #uncommnet if you want to make the output non-deterministic\n",
        "\n",
        "print(\"Bigram:\\n\")\n",
        "print(\"\\n\".join([generate_sent(Bigram, 2) for _ in range(5)]))\n",
        "\n",
        "print(\"\\nTrigram:\\n\")\n",
        "print(\"\\n\".join([generate_sent(Trigram, 3) for _ in range(5)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPmwIHiqvBqL"
      },
      "source": [
        "### [Extra]\n",
        "No points for this\n",
        "\n",
        "For how many of the histories $w_{k-N+1}\\dots w_{k-1}$ is the number of continuations $w_n$ equal to **one**? Calculate the percentage of such cases for the different orders *N*.\n",
        "\n",
        "And which history has the **most possible continuations**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYGPyRyRc91S"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmREq7Zhc91S"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDjAQzkkvBqM"
      },
      "source": [
        "## Ex 2.3 Smoothing effect [5pt]\n",
        "\n",
        "Let $V$ denote our vocabulary. Recall that for any $w \\in V$ `bigram[w]` defines a conditional probability $p(v|w)$ over $v$ in $V$. In the case of an **unsmoothed bigram**, $p(v|w) = 0$ for most $v\\in V$, whereas in the **smoothed bigram** smoothing took care that $p(v|w) > 0$ for *all* $v$.\n",
        "\n",
        "The function `plot_bigram_dist(word, bigram, smoothbigram, k=30)` plots $p(v|word)$ for the `k` and words $v$. One bar shows the probabilities in `bigram` and one in `smoothbigram`. \n",
        "\n",
        "1. Use `plot_bigram_dist` to plot the bigram distributions for two contrasting words $w_1$ and $w_2$, i.e., the distributions of $p(v|w_1)$ and $p(v|w_2)$. Adjust the cut-off `n` parameter for better visualization.\n",
        "\n",
        "2. **Compare** the effect of `k=1` smoothing on the bigram distributions of the frequent $w_1$ and infrequent $w_2$ words. Give an informative insight instead of literally describing the plots. \n",
        "\n",
        "3. Now plot the same distributions but with $k$ being much smaller than 1 (but greater than 0!). **Explain and contrast** how the value of $k$ affects the distributions of the frequent $w_1$ and infrequent $w_2$ words.\n",
        "\n",
        "\n",
        "**[Hint]** Remember that add-1 smoothing turns \n",
        "$$P(w_n\\mid w_{n-1}) = \\frac{C(w_{n-1}w_{n})}{C(w_{n-1})}$$\n",
        "into\n",
        "$$P_{add-1}(w_n\\mid w_{n-1}) = \\frac{C(w_{n-1}w_{n}) + 1}{C(w_{n-1}) + |V|}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7RiiRRxvBqM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns    \n",
        "\n",
        "def plot_bigram_dist(word, bigram, smoothbigram, n=30):\n",
        "    d = bigram[word]\n",
        "    #print(type(d))\n",
        "    ds = smoothbigram[word]\n",
        "    \n",
        "    # sort the probabilities\n",
        "    d_sort = sorted(d.items(), reverse=True, key=lambda t: t[1])[0:n]\n",
        "    ds_sort = sorted(ds.items(), reverse=True, key=lambda t: t[1])[0:n]\n",
        "    \n",
        "    _, probs = zip(*d_sort)\n",
        "    smooth_ws, smooth_probs = zip(*ds_sort)\n",
        "    \n",
        "    # make up for the fact that in the unsmoothed case non-0 probs is generally less than n long\n",
        "    probs = probs + (0,) * (n-len(probs)) \n",
        "\n",
        "    w_data = pd.DataFrame({f\"{word}\": smooth_ws * 2,\n",
        "                           f\"P(w|{word})\": probs + smooth_probs,\n",
        "                           \"smoothing\": [\"unsmoothed\"]*n + [\"smoothed\"]*n})\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plt.xticks(rotation=90)\n",
        "    g = sns.barplot(ax=ax, x=f\"{word}\", y=f\"P(w|{word})\", hue=\"smoothing\",\n",
        "                    data=w_data, palette=\"Blues_d\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKH3i7FTvBqN"
      },
      "outputs": [],
      "source": [
        "# Use these variables\n",
        "Freq_word, Infreq_word = None, None\n",
        "\n",
        "## YOUR CODE HERE ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48XochmkvBqO"
      },
      "outputs": [],
      "source": [
        "# Use this K value and reuse Freq_word, Infreq_word\n",
        "Small_k = 0.01\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN2uU1PxvBqN"
      },
      "source": [
        "█████ YOUR ANSWER HERE █████"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l9-iEXFvBqP"
      },
      "source": [
        "## Ex 2.4 Train sent. prob. [5pt] \n",
        "\n",
        "**Recall** that if we have a sentence $w_1,\\dots,w_n$ we can write\n",
        "\n",
        "$$P(w_1\\dots w_n) = P(w_1)P(w_2|w_1) \\cdots P(w_n|w_1 \\dots w_{n-1}) \\approx P(w_1)P(w_2|w_1)\\cdots P(w_n|w_{n-(N-1)}\\dots w_{n-1})\\prod_{i=1}^{n} P(w_i|w_{i-(N-1)}\\dots w_{i-1})$$\n",
        "\n",
        "where in the last step we make an $N$-gram approximation of the full conditionals.\n",
        "\n",
        "For example, in the case of a bigram (N=2), the above expression reduces to\n",
        "\n",
        "$$P(w_1 \\dots w_n)\\approx P(w_1)\\prod_{i=2}^{n} P(w_i| w_{i-1}).$$\n",
        "\n",
        "The following sentences are taken from the **training data**. Use your **unsmoothed unigram**, **bigram**, and **trigram** language model to estimate their **probabilities**:\n",
        "\n",
        "    1. Every day was about creating something new .\n",
        "    2. In this machine , a beam of protons and anti-protons are accelerated to near the speed of light and brought together in a collision , producing a burst of pure energy ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdoNkywsvBqQ"
      },
      "outputs": [],
      "source": [
        "def sent_prob(sent, lm, N):\n",
        "    \"\"\"\n",
        "    Calculates probability of sent based on the N-gram language mode lm\n",
        "    :param sent: sentence string\n",
        "    :param lm: N-gram language model\n",
        "    :param N: N-gram size\n",
        "    :returns: probabilitity number\n",
        "    \"\"\"\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fIwnQCkc91V"
      },
      "outputs": [],
      "source": [
        "# Print all the abovemnetioned probabilities.\n",
        "# The cell output should make it clear what number stands for which probability. \n",
        "# Reuse the variables Unigram, Unigram_sm, Bigram, Bigram_sm, Trigram, and Trigram_sm.\n",
        "\n",
        "Sents_from_train = (\n",
        "    \"Every day was about creating something new .\", \n",
        "    \"In this machine , a beam of protons and anti-protons are accelerated to near the speed of light and brought together in a collision , producing a burst of pure energy .\"\n",
        ")\n",
        "\n",
        "## YOUR CODE HERE ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repeat** this with the **smoothed (add-1)** versions of the N-grams (in the same above code cell).   \n",
        "**What** is the effect of smoothing on the probabilities and **how** the effect compare across the different $N$-grams models? Provide explanations for the effect and the comparison."
      ],
      "metadata": {
        "id": "uMS37P4azImr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_RnO1_vBqQ"
      },
      "source": [
        "█████ YOUR ANSWER HERE █████"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YFJJ2ThvBqR"
      },
      "source": [
        "## Ex 2.5 Test sent. prob. [5pt]\n",
        "\n",
        "The above sentences were taken from the training set, hence they will all have a probability greater than 0. The big challenge for our language model are of course with the sentences that contain unseen N-grams: if such an N-gram occurs our model immediately assigns the sentence probability zero.\n",
        "\n",
        "The following sentences are taken from the **test set** available in the file **ted-test.txt**.  \n",
        "**Print** the probabilities of the sentences based on the smoothed and unsmoothed language models.\n",
        "\n",
        "    1. Because these robots are really safe .\n",
        "    2. We have sheer nothingness on one side , and we have this vision of a reality that encompasses every conceivable world at the other extreme : the fullest possible reality , nothingness , the simplest possible reality ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3_00-2qvBqR"
      },
      "outputs": [],
      "source": [
        "# Print all the abovemnetioned probabilities.\n",
        "# The cell output should make it clear what number stands for which probability. \n",
        "# Reuse the variables Unigram, Unigram_sm, Bigram, Bigram_sm, Trigram, and Trigram_sm.\n",
        "\n",
        "Sents_from_test = (\n",
        "    \"Because these robots are really safe .\", \n",
        "    \"We have sheer nothingness on one side , and we have this vision of a reality that encompasses every conceivable world at the other extreme : the fullest possible reality , nothingness , the simplest possible reality .\"\n",
        ")\n",
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contrast** the smoothed and unsmoothed probabilities and **explain** the drastic changes (if any) in terms of unseen tokens and sequences."
      ],
      "metadata": {
        "id": "yXJZrc6bD3X-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy10zAGgvBqR"
      },
      "source": [
        "█████ YOUR ANSWER HERE █████"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x4nV5NVvBqS"
      },
      "source": [
        "### [Extra]\n",
        "\n",
        "**Optional** What percentage of the sentences in the test set get assigned probability 0 under your smoothed and unsmoothed language models? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RZY0PfSlvBqS"
      },
      "outputs": [],
      "source": [
        "### ANSWER HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HXaUKivBqS"
      },
      "source": [
        "## Ex 2.6 Perplexity [5pt]\n",
        "\n",
        "**Perplexity** is very frequently used **metric** for evaluating probabilistic models such as language models. The perplexity (sometimes called **PP** for short) of a language model on a sentence is the **inverse probability** of the sentence, **normalized** by the number of words:\n",
        "\n",
        "$$PP(w_1 \\dots w_n) = P(w_1\\dots w_n)^{-\\frac{1}{n}}.$$\n",
        "\n",
        "Here we can again approximate $P(w_1 \\dots w_n)$ with N-gram probabilities, as above.\n",
        "Note: $(x_1\\cdots x_n)^{-\\frac{1}{n}}$ is the **geometric mean** of the numbers $x_1,\\dots,x_n$. It is like the (regular) arithmetic mean, but with **products** instead of **sums**. The geometric mean is a more natural choice in the case of *PP* because behind $P(w_1\\dots w_n)$ is a series of $n$ products ([more here](https://en.wikipedia.org/wiki/Geometric_mean)).\n",
        "\n",
        "\n",
        "\n",
        "Compute the perplexity of the sentences in the training data (from exercise 2.1) based on the smoothed bigram language model.  \n",
        "What big difference between the **probabilities** of the sentences and the **perplexities** of the sentences do you notice?  \n",
        "**Provide** the evidence (can be visualized too) and **explain** the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kPgFt0SvBqS"
      },
      "outputs": [],
      "source": [
        "# Feel free to reuse functions and variables form the previous exercises\n",
        "\n",
        "### YOUR CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bWMxWTwvBqT"
      },
      "source": [
        "█████ YOUR ANSWER HERE █████"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lZOzaX5vBqT"
      },
      "source": [
        "# That's it!\n",
        "\n",
        "Congratulations, you have made it to the end of the tutorial. Here we will recap the gist of this notebook. \n",
        "\n",
        "**Make sure all your cells can be executed and all your answers are there. Then, read on if you're interested!**\n",
        "\n",
        "-----\n",
        "\n",
        "By now you should have a solid feeling for the problem of **sparsity in language data**; there's just never enough data. For the task of language modelling, we saw that sparsity is a serious challenge. \n",
        "\n",
        "It would be great to be able to model $p(w_n|w_1 \\dots w_{n-1})$ for unlimited $n$: the larger $n$ the better our language model should become at capturing the long-range dependencies between words that characterize actual human sentences, and the more probability our model will assign to such sentences as opposed to sentences that are word-soup. But in the N-gram approach, increasing $n$ will quickly kill all generalizing abilities of the model: the model will start to assign probabilities only to sentences it has seen in the training data.\n",
        "\n",
        "So, where to go from here? Here are three directions that we could head in.\n",
        "\n",
        "### Smoothing\n",
        "\n",
        "We have seen one example of smoothing in this lab: add-k smoothing. This is an easy method, both conceptually and implementation-wise. But the results are not great, and the effects it has on the distributions can be extreme.\n",
        "\n",
        "A much more sophisticated method of smoothing is **Kneser-Ney smoothing**. The method is described in detail in section 4.5 of J&M (3rd edition). This is one of the best-performing N-gram smoothing methods, and up to a few years ago, a popular implementation of it called [KenLM](https://kheafield.com/code/kenlm/) gave state-of-the-art results.\n",
        "\n",
        "### From words to characters\n",
        "\n",
        "In this lab we have considered language modeling as the task of predicting a **word** $w_n$ based on a history of **words** $w_1\\cdots w_n$. What if instead we let our basic units of modelling be **characters**? The task then becomes to model $p(c_k\\mid c_{k-N-1}\\dots c_{k-1})$ where each $c_i$ is now an ASCII character instead of an entire word.\n",
        "\n",
        "Suddenly sparsity of data is no longer a problem! The set of characters to use is tiny (< 100) compared to even a small-sized vocabulary as today. Have a look at this very illustrative notebook written by Yoav Golberg to see such a method in action: [The unreasonable effectiveness of Character-level Language Models](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139).\n",
        "\n",
        "(So what is the downside?)\n",
        "\n",
        "\n",
        "### Neural language models\n",
        "\n",
        "The above notebook was actually written as a response to this blog post by Andrej Karpathy: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Go ahead and read it if you haven't already: it is a superb introduction to the topic of Recurrent Neural Networks.\n",
        "\n",
        "Neural language models solve the problem of data sparsity in a different manner. Instead of estimating the probabilities $p(w_k\\mid w_{k-N-1}\\dots w_{k-1})$ by counting occurrences in the data, they use a neural network $f_{\\theta}$ parametrized by parameters $\\theta$ to predict this probability. The parameters $\\theta$ are learned through optimization. \n",
        "\n",
        "The simplest approach goes like this: each word in the history $w_{k-N-1}\\dots w_{k-1}$ is embedded separately giving vectors $e_{k-N-1}\\dots e_{k-1}$ and then concatenated into one long vector $[e_{k-N-1};\\dots ;e_{k-1}]$. The network then uses this history vector to predict a probability distribution over words $w$ in the vocabulary $V$:\n",
        "\n",
        "$$p(w \\mid w_{k-N-1}\\dots w_{k-1}) = f_{\\theta}([e_{k-N-1};\\dots;e_{k-1}]).$$\n",
        "\n",
        "(In order to produce legitimate probabilities the final layer of such a network will be for example a $softmax$.)\n",
        "\n",
        "This provides a solution to the sparsity problem by having the network let the individual embeddings of the words in the history interact through its non-linear transformation. We are letting the network figure out the smoothing itself!\n",
        "\n",
        "RNNs are a clever extension of this idea, where a hidden state vector $h$ is re-used and updated at each step $k$ in order to store the information of the entire history up to step $k-1$. That is, an RNN actually does away with the N-order approximation; it tries to model the full conditional directly! That means that\n",
        "\n",
        "$$p(w \\mid w_1\\dots w_{k-1}) \\approx RNN_{\\theta}([e_{k-1};h_{k-1}])$$\n",
        "\n",
        "where the hidden state $h_{k-1}$ is a compression of the *entire history* $w_1\\dots w_{k-1}$.\n",
        "\n",
        "Another great place to learn about RNNs, their problems, and solutions to those, is on the blog of [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
        "\n",
        "-----------\n",
        "(And now, it's time to read the classic essay by Eugene Wigner that gave both of the posts their title: [The Unreasonable Effectiveness of Mathematics in the Natural Sciences](http://www.dartmouth.edu/~matc/MathDrama/reading/Wigner.html))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgments\n",
        "\n",
        "Most of this lab was developed in collaboration with Joost Bastings and Dan Douwe.  \n",
        "Later it was revised by a couple of people.  \n",
        "The recent updates by Lasha Abzianidze make the notebook more streamlined and foolproof from the grading and the large course perspectives. "
      ],
      "metadata": {
        "id": "acBrUUTFFdHr"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Em9s4OTHfYda",
        "YbOqa3y5hefa",
        "mPmwIHiqvBqL",
        "9x4nV5NVvBqS"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}